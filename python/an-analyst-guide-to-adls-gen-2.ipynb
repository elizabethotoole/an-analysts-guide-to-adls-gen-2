{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Analyst's Guide to Azure Data Lake Storage Gen 2 \n",
    "Author: [elizabethotoole](https://github.com/elizabethotoole)\n",
    "\n",
    "This beginner-friendly notebook serves as a step-by-step guide for connecting to Azure Data Lake Storage (ADLS) Gen 2 using python üêç.\n",
    "\n",
    "Join me as I walk you through the essentials, so you can focus on analysing data, not battling with storage setups. üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A Quick Overview of Fundamental Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is ADLS Gen 2?\n",
    "Azure Data Lake Storage Gen 2 (ADLS Gen 2) is a cloud storage service from Microsoft Azure designed to store large amounts of data.\n",
    "\n",
    "#### How is it structured?\n",
    "- **Storage account** - The highest level of storage and is where all your data is stored and accessed. <br>\n",
    "    > e.g. \"https://youraccount.blob.core.windows.net\" <br>\n",
    "    >\n",
    "    The youraccount part of the URL should be replaced with your actual Azure Storage account name. The account_url will be used to authenticate and connect to your specific Azure Storage account. <br>\n",
    "\n",
    "- **Containers**- Inside a storage account, data is organised into containers. Think of containers as \"buckets\" that hold your data. <br>\n",
    "    > e.g. \"your-container-name\"\n",
    "    >\n",
    "    Replace \"your-container-name\" with the actual name of the container where your files are stored.\n",
    "\n",
    "- **Folders and Subfolders**: ADLS Gen 2 allows you to organise data using folders, this differs to traditional sotrage which is typically stored in flat containers.\n",
    "    > e.g. \"your-container-name/raw_data/\" <br>\n",
    "    >\n",
    "    You can have folders like raw_data/, processed_data/, or analytics/, and within these, additional subfolders like logs/, reports/, etc. <br>\n",
    "\n",
    "- **Blobs**: These are the files themselves which are stored inside the containers, folders, or subfolders. \n",
    "    > e.g.  \"raw_data/logs/jan_log.csv\"\n",
    "    >\n",
    "    Blobs (aka files) can be in various formats like CSV, Parquet, JSON, and even images and videos. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Downloading data from ALDS Gen 2 ‚¨áÔ∏è\n",
    "Now you know the essentials, let's get into it! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import our packages\n",
    "In this step, we'll import all the required libraries that we'll use to interact with the Azure Blob Storage and process our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from azure.identity import InteractiveBrowserCredential  # Used for authenticating with Azure using the interactive browser method\n",
    "from azure.storage.blob import BlobServiceClient  # Used for connecting to the Azure Blob Storage service\n",
    "\n",
    "import pyarrow as pa \n",
    "import pyarrow.csv as pv_csv  # for csv files\n",
    "import pyarrow.parquet as pq  # for for parquet files\n",
    "import pandas as pd  # Pandas library used for data manipulation and analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Connect to Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Azure Blob Storage\n",
    "account_url = \"https://youraccount.blob.core.windows.net\" # Replace youraccount with the name of your Azure Blob Storage account.\n",
    "container_name = \"your-container-name\" # Replace your-container-name with the name of your container in Azure Blob Storage.\n",
    "default_credential = InteractiveBrowserCredential() # Use the interactive browser method for authenticating with Azure, you'll be prompted to log in via the web browser.\n",
    "\n",
    "blob_service_client = BlobServiceClient(account_url=account_url, credential=default_credential)\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: List all files in the container\n",
    "file_names = [blob.name for blob in container_client.list_blobs()]\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: List files from a certain folder in the container by adding a prefix\n",
    "prefix = \"folder1/\"\n",
    "\n",
    "file_names = [blob.name for blob in container_client.list_blobs(name_starts_with=prefix)]\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: List files with a specific file name\n",
    "specific_file_name = \"folder1/my_file.parquet\"  # Replace with the name of your file\n",
    "\n",
    "file_names = [blob.name for blob in container_client.list_blobs(name_starts_with=specific_file_name)]\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load Data from the Blob Storage\n",
    "Now we've listed the files in Azure Blob Storage, it's crucial to consider the size of your dataset as this will influence how we load the data into a dataframe for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Using Pandas for smaller datasets\n",
    "Pandas is a popular open-source Python library used for data manipulation and analysis. <br>\n",
    "\n",
    "If the dataset fits into memory, using pandas is simple and effective, allowing the data to be stored in a dataframe using rows and columns (like an Excel spreadsheet or SQL table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV Files\n",
    "\n",
    "# List to hold dataframes\n",
    "dfs = []\n",
    "\n",
    "# Download each blob, read as CSV, and append to the dfs list\n",
    "for file_name in file_names:\n",
    "    # Download the blob using the blob client\n",
    "    blob_client = container_client.get_blob_client(file_name)\n",
    "    blob_data = blob_client.download_blob()  # Retrieves the blob data in its raw binary format\n",
    "    \n",
    "    # Convert the blob data into an Arrow Table directly\n",
    "    buffer = pa.BufferReader(blob_data.readall()) # reads the blob data into buffer\n",
    "    table = pv_csv.read_csv(buffer)  # Read CSV directly from buffer\n",
    "    \n",
    "    # Convert the Arrow Table to a pandas DataFrame\n",
    "    df = table.to_pandas()\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "imported_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Now `imported_data` holds the combined DataFrame\n",
    "print(imported_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading parquet files\n",
    "\n",
    "# List to hold dataframes\n",
    "dfs = []\n",
    "\n",
    "# Download each blob, read as Parquet, and append to the dfs list\n",
    "for file_name in file_names:\n",
    "    # Download the blob using the blob client\n",
    "    blob_client = container_client.get_blob_client(file_name)\n",
    "    blob_data = blob_client.download_blob()  # Retrieves the blob data in its raw binary format\n",
    "    \n",
    "    # Read the Parquet data directly using pyarrow\n",
    "    buffer = pa.BufferReader(blob_data.readall())  # reads the blob data into buffer\n",
    "    table = pq.read_table(buffer)  # Read the Parquet data into an Arrow Table\n",
    "    \n",
    "    # Convert the Arrow Table to a pandas DataFrame\n",
    "    df = table.to_pandas()\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "imported_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Now `imported_data` holds the combined DataFrame\n",
    "print(imported_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Using Spark for larger datasets\n",
    "PySpark is the Python API for Apache Spark, an open-source, distributed computing system that is widely used for big data processing and analytics. Apache Spark allows you to process large amounts of data across a distributed computing cluster in parallel, making it highly scalable so allows us to handle datasets that don't fit into a single machine's memory.\n",
    "\n",
    "For datasets that exceed your system's memory, loading them with pandas may cause memory errors. In such cases, you can use Pyspark instead. <br>\n",
    "\n",
    "As an analyst, you're more likely to access very large datasets through dedicated environments such Databricks where your fellow data engineers have done a lot of the heavy lifting with spark configurations. Below I have provided code to help utilise Pyspark and connecting to ADLS Gen 2 storage within Databricks.\n",
    "\n",
    "Note - While explaining spark configurations is beyond the scope of this beginner's guide, there are some useful resources listed below for more information. <br>\n",
    "> If you're interested in learning more take a look at this useful article: <br>\n",
    "> https://subhamkharwal.medium.com/pyspark-connect-azure-adls-gen-2-c4efa5bf016b  <br>\n",
    "> The code is also available via the author's github: <br>\n",
    "> https://github.com/subhamkharwal/ease-with-apache-spark/blob/master/30_connect_adls_gen2.ipynb  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading data from ADLS Gen 2 within the Databricks environment or similar** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For use within the databricks environment\n",
    "# A function to load data from Azure Data Lake Storage Gen 2 into a Spark DataFrame\n",
    "# This function supports reading Parquet and CSV files from ADLS Gen 2 but could be expanded to support other file formats.\n",
    "\n",
    "def load_lake_data(file_path, file_format):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Loads data stored in Azure Data Lake Storage Gen 2 (ADLS Gen 2) into a Spark DataFrame.\n",
    "\n",
    "    This function supports reading Parquet and CSV files from ADLS Gen 2 and applies the necessary \n",
    "    configurations like treating the first row as headers for CSV files, inferring schema, and \n",
    "    recursively looking for files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The full path to the file or directory in ADLS storage. This should be \n",
    "                            in the format `abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-file>`.\n",
    "        file_format (str): The format of the file to read. Valid options are \"parquet\" or \"csv\".\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: A Spark DataFrame containing the data read from the specified file format.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If an unsupported file format is provided.\n",
    "\n",
    "    Example usage:\n",
    "        file_path = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-file>\"\n",
    "        df = load_lake_data(file_path, \"parquet\")\n",
    "        df = load_lake_data(file_path, \"csv\")\n",
    "    \"\"\"\n",
    "\n",
    "    from databricks.sdk.runtime import spark\n",
    "\n",
    "    if file_format == \"parquet\":\n",
    "        output = (\n",
    "            spark.read\n",
    "            .option(\"header\", \"true\")  # Treat the first row as column names\n",
    "            .option(\"recursiveFileLookup\", \"true\")  # Look for files recursively\n",
    "            .parquet(file_path)  # Read Parquet data from the specified path\n",
    "        )\n",
    "\n",
    "    elif file_format == \"csv\":\n",
    "        output = (\n",
    "            spark.read.option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .csv(file_path)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "\n",
    "    return output     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Congrats, now you can use your dataframe for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a quick look at your dataframe:\n",
    "print(imported_data.head())\n",
    "\n",
    "# In databricks:\n",
    "display(imported_data.limit(5))  # Display the first 5 rows interactively in Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Uploading Data to Azure Blob Storage ‚¨ÜÔ∏è\n",
    "\n",
    "The below code allows us to upload a file from our local machine into Azure Blob Storage which we can then subsequently incorporate into analytical projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide lake details for where you want to upload the file \n",
    "\"\"\"\n",
    "The below have been defined earlier on in this notebook but are provided within this comment for easy reference\n",
    "account_url = \"https://youraccount.blob.core.windows.net\" \n",
    "container_name = \"your-container-name\" \n",
    "\n",
    "default_credential = InteractiveBrowserCredential()\n",
    "blob_service_client = BlobServiceClient(account_url=account_url, credential=default_credential) \n",
    "container_client = blob_service_client.get_container_client(container_name) \n",
    "\"\"\"\n",
    "\n",
    "# Provide the local file path and desired destination blob name (include the file and file tpye)\n",
    "local_file_path = \"Documents/file_1.csv\"  # Path to the local CSV file you want to upload\n",
    "blob_name = \"uploads/file1.csv\"  # Path in the container\n",
    "\n",
    "# Now upload the file to Azure Blob Storage\n",
    "try:\n",
    "    # Open the file in binary mode\n",
    "    with open(local_file_path, mode=\"rb\") as data:\n",
    "        # Upload the file to Azure Blob Storage\n",
    "        container_client.upload_blob(blob_name, data, overwrite=True) #overwrite=True will replace the file if it already exists\n",
    "        print(f\"File {local_file_path} uploaded successfully to {container_name}/{blob_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
